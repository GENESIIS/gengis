{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":113143,"status":"ok","timestamp":1719461534748,"user":{"displayName":"Dilshani RUBASINGHE [GENESIIS]","userId":"13675050694185277063"},"user_tz":-330},"id":"gD6KZSyUPLN6","outputId":"6b9916ec-76ab-406e-9e58-751a4d9c7aca"},"outputs":[],"source":["!pip install segmentation_models_pytorch"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":21639,"status":"ok","timestamp":1719461559922,"user":{"displayName":"Dilshani RUBASINGHE [GENESIIS]","userId":"13675050694185277063"},"user_tz":-330},"id":"OHEikXo-PxcK","outputId":"48ad103c-7ce7-4fac-f63e-cfba17e7e19b"},"outputs":[],"source":["!pip install rasterio"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":17490,"status":"ok","timestamp":1719461579885,"user":{"displayName":"Dilshani RUBASINGHE [GENESIIS]","userId":"13675050694185277063"},"user_tz":-330},"id":"cUgaID23P10a","outputId":"a42cb2a1-cb57-4c0b-cd1d-287412854af5"},"outputs":[],"source":["!pip install torch"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":18091,"status":"ok","timestamp":1719461600197,"user":{"displayName":"Dilshani RUBASINGHE [GENESIIS]","userId":"13675050694185277063"},"user_tz":-330},"id":"HogckN_KP-Wh","outputId":"92dae1e7-d600-455e-ba14-0091bce11ab6"},"outputs":[],"source":["!pip install patchify"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":26517,"status":"ok","timestamp":1719461628737,"user":{"displayName":"Dilshani RUBASINGHE [GENESIIS]","userId":"13675050694185277063"},"user_tz":-330},"id":"BAE55QQSPA97","outputId":"ec72cb5f-47ab-4c2d-e13f-bf2d75aba50f"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WUCnRs1F_Fkv"},"outputs":[],"source":["import os\n","os.environ['TF_ENABLE-ONEDNN-OPTS'] ='0'\n","import torch\n","import numpy as np\n","from  matplotlib  import pyplot as plt\n","from patchify import patchify, unpatchify\n","import os\n","os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n","import torch\n","import numpy as np\n","from matplotlib import pyplot as plt\n","from patchify import patchify, unpatchify\n","from PIL import Image\n","from sklearn.preprocessing import MinMaxScaler\n","from segmentation_models_pytorch import Unet\n","import segmentation_models_pytorch.utils\n","import torch\n","import rasterio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X8Gmv5IbQQRo"},"outputs":[],"source":["# Function\n","\n","#to convert label to RGB\n","\n","def label_to_rgb(mask):\n","    background = np.array([0, 0, 0])  # Background\n","    building = np.array([60, 16, 152])  # Building\n","    woodland = np.array([132, 41, 246])  # Woodland\n","    water = np.array([226, 169, 41])  # Water\n","\n","    segmented_img = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n","    segmented_img[mask == 0] = background\n","    segmented_img[mask == 1] = building\n","    segmented_img[mask == 2] = woodland\n","    segmented_img[mask == 3] = water\n","    return segmented_img"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qvAsleg3QU_Y"},"outputs":[],"source":["\n","image_path = \"/content/drive/MyDrive/Semantic_segmentation_dataset2/images/M-33-32-B-b-4-4.tif\"\n","with rasterio.open(image_path) as src:\n","    img = src.read()\n","    img = np.moveaxis(img, 0, -1)  # Convert from (bands, rows, cols) to (rows, cols, bands)\n","    transform = src.transform  # Save the transform information\n","    crs = src.crs  # Save the CRS information"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":2209,"status":"ok","timestamp":1719461780989,"user":{"displayName":"Dilshani RUBASINGHE [GENESIIS]","userId":"13675050694185277063"},"user_tz":-330},"id":"hWmAfYLuQfkT","outputId":"dc87e747-fc9f-4ab7-9cf3-3388325dff95"},"outputs":[],"source":["\n","model_path = \"/content/drive/MyDrive/model/trained_landcover_unet_efficientnet-b0_epochs18_patch512_batch16 (2).pth\"\n","model = torch.load(model_path, map_location=torch.device('cpu'))\n","model.eval()  # Set the model to evaluation mode\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UrRBDCYpQlJw"},"outputs":[],"source":["# Function to make predictions\n","def predict_patch(model, patch):\n","    patch = torch.tensor(patch).permute(2, 0, 1).unsqueeze(0).float()  # Convert to torch tensor and add batch dimension\n","    with torch.no_grad():\n","        pred = model(patch)\n","    pred = torch.argmax(pred, dim=1).squeeze().numpy()\n","    return pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TQOplQ2MQoOq"},"outputs":[],"source":["\n","def predict_patch(model, patch):\n","    patch = torch.tensor(patch).permute(2, 0, 1).unsqueeze(0).float()  # Convert to torch tensor and add batch dimension\n","    with torch.no_grad():\n","        pred = model(patch)\n","    pred = torch.argmax(pred, dim=1).squeeze().numpy()\n","    return pred\n","if img.dtype != np.uint8:\n","    img = img.astype(np.uint8)\n","if img.ndim == 2:  # Grayscale image\n","    img = np.stack((img,)*3, axis=-1)  # Convert to RGB by duplicating channels\n","elif img.ndim == 3 and img.shape[2] == 1:  # Single channel grayscale image\n","    img = np.concatenate([img]*3, axis=2)  # Convert to RGB\n","\n","patch_size = 512\n","SIZE_X = (img.shape[1] // patch_size) * patch_size\n","SIZE_Y = (img.shape[0] // patch_size) * patch_size\n","\n","\n","large_img = img[:SIZE_Y, :SIZE_X]\n","\n","# Convert to PIL Image and back to numpy array to ensure compatibility\n","large_img_pil = Image.fromarray(large_img, 'RGB')\n","large_img = np.array(large_img_pil)\n","\n","patches_img = patchify(large_img, (patch_size, patch_size, 3), step=patch_size)\n","patches_img = patches_img[:, :, 0, :, :, :]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A6XwemgpQq9T"},"outputs":[],"source":["\n","scaler = MinMaxScaler()\n","# Predict patch by patch\n","patched_prediction = []\n","for i in range(patches_img.shape[0]):\n","    for j in range(patches_img.shape[1]):\n","        single_patch_img = patches_img[i, j, :, :, :]\n","        single_patch_img = scaler.fit_transform(single_patch_img.reshape(-1, single_patch_img.shape[-1])).reshape(single_patch_img.shape)\n","        pred = predict_patch(model, single_patch_img)\n","        patched_prediction.append(pred)\n","\n","patched_prediction = np.array(patched_prediction)\n","patched_prediction = np.reshape(patched_prediction, [patches_img.shape[0], patches_img.shape[1], patch_size, patch_size])\n","\n","unpatched_prediction = unpatchify(patched_prediction, (large_img.shape[0], large_img.shape[1]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nNa2F4ZRQv9E"},"outputs":[],"source":["# Convert predicted labels to RGB\n","prediction_rgb = label_to_rgb(unpatched_prediction)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zdbhb8bsSJ9b"},"outputs":[],"source":["# save the predicted tif\n","output_path = \"/content/drive/MyDrive/Semantic_segmentation_dataset2/output/predicted_mask_coordinates114.tif\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4349,"status":"ok","timestamp":1719462437476,"user":{"displayName":"Dilshani RUBASINGHE [GENESIIS]","userId":"13675050694185277063"},"user_tz":-330},"id":"03vri0b0SREH","outputId":"fd2dca76-8649-40e6-cf2e-a6cf8ab2fa55"},"outputs":[],"source":["\n","with rasterio.open(\n","    output_path,\n","    'w',\n","    driver='GTiff',\n","    height=prediction_rgb.shape[0],\n","    width=prediction_rgb.shape[1],\n","    count=3,  # Number of channels\n","    dtype=prediction_rgb.dtype,\n","    crs=crs,  # Use the saved CRS information\n","    transform=transform  # Use the saved transform information\n",") as dst:\n","    dst.write(prediction_rgb[:, :, 0], 1)  # Red channel\n","    dst.write(prediction_rgb[:, :, 1], 2)  # Green channel\n","    dst.write(prediction_rgb[:, :, 2], 3)  # Blue channel\n","\n","print(f\"Predicted mask saved to {output_path}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uoTI9dHHCfod"},"outputs":[],"source":["def plot_segmented_image_with_legend(segmented_image):\n","    labels = ['Background', 'Building', 'Woodland', 'Water', 'Road']\n","    colors = [(0, 0, 0), (60, 16, 152), (132, 41, 246), (226, 169, 41), (128, 64, 128)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SpXik0zlCYV9"},"outputs":[],"source":["def label_to_rgb(predicted_image):\n","    Building = np.array([0, 0, 0])  # Background\n","    Building = np.array([60, 16, 152])  # Building\n","    Vegetation = np.array([132, 41, 246])  # Woodland\n","    Water = np.array([226, 169, 41])  # Water\n","\n","    Unlabeled = np.array([155, 155, 155])\n","\n","    segmented_img = np.zeros((predicted_image.shape[0], predicted_image.shape[1], 3), dtype=np.uint8)\n","    segmented_img[predicted_image == 0] = Building\n","    segmented_img[predicted_image == 1] = Vegetation\n","    segmented_img[predicted_image == 2] = Water\n","    segmented_img[predicted_image == 3] = Unlabeled\n","    return segmented_img\n","\n","prediction_without_smooth_blending = label_to_rgb(unpatched_prediction)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"executionInfo":{"elapsed":21606,"status":"ok","timestamp":1719462478538,"user":{"displayName":"Dilshani RUBASINGHE [GENESIIS]","userId":"13675050694185277063"},"user_tz":-330},"id":"hJ9m2LXsSVu2","outputId":"3f4ced3a-9ecf-41a6-aee0-c7340374658c"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","if img.dtype != np.uint8:\n","    img = img.astype(np.uint8)\n","if img.ndim == 2:  # Grayscale image\n","    img = np.stack((img,)*3, axis=-1)  # Convert to RGB by duplicating channels\n","elif img.ndim == 3 and img.shape[2] == 1:  # Single channel grayscale image\n","    img = np.concatenate([img]*3, axis=2)  # Convert to RGB\n","if img.shape[2] > 3:\n","    img = img[:, :, :3]\n","if prediction_rgb.ndim == 2:  # Grayscale prediction\n","    prediction_rgb = np.stack((prediction_rgb,)*3, axis=-1)  # Convert to RGB by duplicating channels\n","elif prediction_rgb.ndim == 3 and prediction_rgb.shape[2] == 1:  # Single channel prediction\n","    prediction_rgb = np.concatenate([prediction_rgb]*3, axis=2)  # Convert to RGB\n","\n","\n","if prediction_rgb.shape[2] > 3:\n","    prediction_rgb = prediction_rgb[:, :, :3]\n","\n","# Display results\n","plt.figure(figsize=(12, 12))\n","plt.subplot(221)\n","plt.title('Testing Image')\n","plt.imshow(img)\n","\n","plt.subplot(223)\n","plt.title('Prediction without smooth blending')\n","plt.imshow(prediction_rgb)\n","\n","plt.show()\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNyavEHmP3hPOxtjmX6+8B3","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
