{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":23708,"status":"ok","timestamp":1719808805147,"user":{"displayName":"Dilshani RUBASINGHE [GENESIIS]","userId":"13675050694185277063"},"user_tz":-330},"id":"QNjEz7-esZp4","outputId":"37d1f919-4e7d-4b15-92cb-ece38eaf4ca0"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":7041,"status":"ok","timestamp":1719808815344,"user":{"displayName":"Dilshani RUBASINGHE [GENESIIS]","userId":"13675050694185277063"},"user_tz":-330},"id":"XxMZjzQTnfUE","outputId":"fa112654-8b0b-4e5a-c686-a13dc79c4f44"},"outputs":[],"source":["!pip install patchify"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":96171,"status":"ok","timestamp":1719808914243,"user":{"displayName":"Dilshani RUBASINGHE [GENESIIS]","userId":"13675050694185277063"},"user_tz":-330},"id":"r5_U2SZAnnJ0","outputId":"f789bf4b-7f9d-464e-bff5-925157f5f5e3"},"outputs":[],"source":["!pip install segmentation segmentation_models_pytorch"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1021,"status":"ok","timestamp":1719809037818,"user":{"displayName":"Dilshani RUBASINGHE [GENESIIS]","userId":"13675050694185277063"},"user_tz":-330},"id":"Ea0tYZfsoE4E","outputId":"b534309d-9b7d-4f7a-c05e-de6f208dd879"},"outputs":[],"source":["!pip install segmentation segmentation_models_pytorch.utils"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":14864,"status":"ok","timestamp":1719809060210,"user":{"displayName":"Dilshani RUBASINGHE [GENESIIS]","userId":"13675050694185277063"},"user_tz":-330},"id":"6LdwPNhjm3-2"},"outputs":[],"source":["import os\n","os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n","import torch\n","import numpy as np\n","from matplotlib import pyplot as plt\n","from patchify import patchify, unpatchify\n","from PIL import Image\n","from sklearn.preprocessing import MinMaxScaler\n","from segmentation_models_pytorch import Unet\n","import segmentation_models_pytorch.utils\n","import torch"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":411,"status":"ok","timestamp":1719809063838,"user":{"displayName":"Dilshani RUBASINGHE [GENESIIS]","userId":"13675050694185277063"},"user_tz":-330},"id":"I8KMlT_UobMR","outputId":"fd1d108f-85db-4afe-8b78-cd9b84e7a8f7"},"outputs":[],"source":["! mkdir ~/.kaggle\n","! cp kaggle.json ~/.kaggle/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":391,"status":"ok","timestamp":1719809067630,"user":{"displayName":"Dilshani RUBASINGHE [GENESIIS]","userId":"13675050694185277063"},"user_tz":-330},"id":"NIH8ZtBrok1R","outputId":"7cc0ad41-0a88-425e-e262-0b4e03d45bd6"},"outputs":[],"source":["! chmod 600 ~/.kaggle/kaggle.json"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":19070,"status":"ok","timestamp":1719809089198,"user":{"displayName":"Dilshani RUBASINGHE [GENESIIS]","userId":"13675050694185277063"},"user_tz":-330},"id":"We2LN-Mhop7J","outputId":"72d09454-ae7c-4dd7-fe3f-e336cee02997"},"outputs":[],"source":["! kaggle datasets download -d adrianboguszewski/landcoverai"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":18573,"status":"ok","timestamp":1719809110956,"user":{"displayName":"Dilshani RUBASINGHE [GENESIIS]","userId":"13675050694185277063"},"user_tz":-330},"id":"CpRSCEmJpeQo","outputId":"ccf90590-56df-4c69-f215-73efecfb198e"},"outputs":[],"source":["! mkdir data\n","! unzip landcoverai.zip -d data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"executionInfo":{"elapsed":19106,"status":"ok","timestamp":1719809134014,"user":{"displayName":"Dilshani RUBASINGHE [GENESIIS]","userId":"13675050694185277063"},"user_tz":-330},"id":"w6TETqV4pwhu","outputId":"f3868bd5-89f5-475e-8396-bd4560c3c16f"},"outputs":[],"source":["import os\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","print(\"Image and Mask filename: M-33-20-D-c-4-2.tif\")\n","print()\n","temp_img = cv2.imread(\"data/images/M-33-20-D-c-4-2.tif\", 1) # 3 channels / spectral bands\n","temp_img = cv2.cvtColor(temp_img, cv2.COLOR_BGR2RGB)\n","print(\"Image shape:\", temp_img.shape)\n","print()\n","\n","plt.figure(figsize=(12, 8))\n","plt.subplot(121)\n","plt.imshow(temp_img[:,:,0])\n","plt.title(\"One channel of image\")\n","plt.subplot(122)\n","plt.imshow(temp_img)\n","plt.title(\"All channels of image\")\n","plt.show()\n","print()\n","\n","temp_mask = cv2.imread(\"data/masks/M-33-20-D-c-4-2.tif\") # 3 channels but all same. Can also read with cv2.imread(path, 0) to get only one channel.\n","print(\"Mask shape:\", temp_mask[:,:,0].shape)\n","print()\n","classes, count = np.unique(temp_mask[:,:,0], return_counts=True) # Visualize only one channel. All chanels are identical.\n","print(\"Classes are: \", classes, \" and the counts are: \", count)\n","print()\n","\n","plt.figure(figsize=(6, 4))\n","plt.imshow(temp_mask[:,:,0])\n","plt.title(\"Mask\")\n","plt.show()\n","print()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":288625,"status":"ok","timestamp":1719809425037,"user":{"displayName":"Dilshani RUBASINGHE [GENESIIS]","userId":"13675050694185277063"},"user_tz":-330},"id":"CVoy9LU4qEhz","outputId":"14e4ac95-95d9-4c56-ca2d-830e0d253bac"},"outputs":[],"source":["# Patching the images\n","from patchify import patchify\n","from PIL import Image\n","\n","root_directory = \"data\"\n","\n","img_dir = os.path.join(root_directory, \"images\")\n","mask_dir = os.path.join(root_directory, \"masks\")\n","\n","patch_size = 512\n","\n","patches_img_dir = os.path.join(f\"patches_{patch_size}\", \"images\")\n","patches_img_dir = os.path.join(root_directory, patches_img_dir)\n","os.makedirs(patches_img_dir, exist_ok=True)\n","patches_mask_dir = os.path.join(f\"patches_{patch_size}\", \"masks\")\n","patches_mask_dir = os.path.join(root_directory, patches_mask_dir)\n","os.makedirs(patches_mask_dir, exist_ok=True)\n","\n","\n","def patching(data_dir, patches_dir, patch_size):\n","  for filename in os.listdir(data_dir):\n","    if filename.endswith('.tif'):\n","      img = cv2.imread(os.path.join(data_dir, filename), 1)\n","      max_height = (img.shape[0] // patch_size) * patch_size\n","      max_width = (img.shape[1] // patch_size) * patch_size\n","      img = img[0:max_height, 0:max_width]\n","      print(f\"Patchifying {filename}...\")\n","      patches = patchify(img, (patch_size, patch_size, 3), step = patch_size)  # non-overlapping\n","      print(\"Patches shape:\", patches.shape)\n","      for i in range(patches.shape[0]):\n","        for j in range(patches.shape[1]):\n","          single_patch = patches[i, j, 0, :, :] # the 0 is an extra unncessary dimension added by patchify for multiple channels scenario\n","          cv2.imwrite(os.path.join(patches_dir, filename.replace(\".tif\", f\"_patch_{i}_{j}.tif\")), single_patch)\n","\n","print()\n","print(\"Dividing images into patches...\")\n","patching(img_dir, patches_img_dir, patch_size)\n","print(\"Dividing images into patches completed successfull!\")\n","\n","print()\n","print(\"Dividing masks into patches...\")\n","patching(mask_dir, patches_mask_dir, patch_size)\n","print(\"Dividing masks into patches completed successfull!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":376,"status":"ok","timestamp":1719809429082,"user":{"displayName":"Dilshani RUBASINGHE [GENESIIS]","userId":"13675050694185277063"},"user_tz":-330},"id":"IrgGO0r1rYJC","outputId":"f4204bfd-7967-418b-91c9-946632f1e6da"},"outputs":[],"source":["print(len(os.listdir(patches_img_dir)))\n","print(len(os.listdir(patches_mask_dir)))"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":261907,"status":"ok","timestamp":1719809693659,"user":{"displayName":"Dilshani RUBASINGHE [GENESIIS]","userId":"13675050694185277063"},"user_tz":-330},"id":"zmv1NwNbrdjZ"},"outputs":[],"source":["def discard_useless_patches(patches_img_dir, patches_mask_dir):\n","  for filename in os.listdir(patches_mask_dir):\n","    img_path = os.path.join(patches_img_dir, filename)\n","    mask_path = os.path.join(patches_mask_dir, filename)\n","    img = cv2.imread(img_path)\n","    mask = cv2.imread(mask_path)\n","    classes, count = np.unique(mask, return_counts = True)\n","    \n","    if (count[0] / count.sum()) > 0.95:\n","      os.remove(img_path)\n","      os.remove(mask_path)\n","\n","discard_useless_patches(patches_img_dir, patches_mask_dir)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":394,"status":"ok","timestamp":1719809715527,"user":{"displayName":"Dilshani RUBASINGHE [GENESIIS]","userId":"13675050694185277063"},"user_tz":-330},"id":"WQqcK1FXvptI","outputId":"cb10e684-8661-4c19-b171-bb09173bfe6c"},"outputs":[],"source":["print(len(os.listdir(patches_img_dir)))\n","print(len(os.listdir(patches_mask_dir)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":475865,"status":"ok","timestamp":1719810300582,"user":{"displayName":"Dilshani RUBASINGHE [GENESIIS]","userId":"13675050694185277063"},"user_tz":-330},"id":"Y5zNNE9rpbin","outputId":"b8fa01a8-defd-461d-d10c-e17b401923f6"},"outputs":[],"source":["import os\n","import glob\n","\n","import cv2\n","\n","IMGS_DIR = \"/content/drive/MyDrive/Semantic_segmentation_dataset2/images\"\n","MASKS_DIR = \"/content/drive/MyDrive/Semantic_segmentation_dataset2/masks\"\n","OUTPUT_DIR = \"/content/drive/MyDrive/Semantic_segmentation_dataset2/output/github.tif\"\n","\n","TARGET_SIZE = 512\n","\n","img_paths = glob.glob(os.path.join(IMGS_DIR, \"*.tif\"))\n","mask_paths = glob.glob(os.path.join(MASKS_DIR, \"*.tif\"))\n","\n","img_paths.sort()\n","mask_paths.sort()\n","\n","os.makedirs(OUTPUT_DIR)\n","for i, (img_path, mask_path) in enumerate(zip(img_paths, mask_paths)):\n","    img_filename = os.path.splitext(os.path.basename(img_path))[0]\n","    mask_filename = os.path.splitext(os.path.basename(mask_path))[0]\n","    img = cv2.imread(img_path)\n","    mask = cv2.imread(mask_path)\n","\n","    assert img_filename == mask_filename and img.shape[:2] == mask.shape[:2]\n","\n","    k = 0\n","    for y in range(0, img.shape[0], TARGET_SIZE):\n","        for x in range(0, img.shape[1], TARGET_SIZE):\n","            img_tile = img[y:y + TARGET_SIZE, x:x + TARGET_SIZE]\n","            mask_tile = mask[y:y + TARGET_SIZE, x:x + TARGET_SIZE]\n","\n","            if img_tile.shape[0] == TARGET_SIZE and img_tile.shape[1] == TARGET_SIZE:\n","                out_img_path = os.path.join(OUTPUT_DIR, \"{}_{}.jpg\".format(img_filename, k))\n","                cv2.imwrite(out_img_path, img_tile)\n","\n","                out_mask_path = os.path.join(OUTPUT_DIR, \"{}_{}_m.png\".format(mask_filename, k))\n","                cv2.imwrite(out_mask_path, mask_tile)\n","\n","            k += 1\n","\n","    print(\"Processed {} {}/{}\".format(img_filename, i + 1, len(img_paths)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":409,"status":"ok","timestamp":1719811916535,"user":{"displayName":"Dilshani RUBASINGHE [GENESIIS]","userId":"13675050694185277063"},"user_tz":-330},"id":"8n-JmbQhxmie"},"outputs":[],"source":["\n","\n","print(\"Patched images and masks are saved in:\", OUTPUT_DIR)"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":376,"status":"ok","timestamp":1719811226220,"user":{"displayName":"Dilshani RUBASINGHE [GENESIIS]","userId":"13675050694185277063"},"user_tz":-330},"id":"Hbo7pew3u7yw"},"outputs":[],"source":["splitfolder=\"/content/drive/MyDrive/Semantic_segmentation_dataset2/split_folder\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8049,"status":"ok","timestamp":1719811524571,"user":{"displayName":"Dilshani RUBASINGHE [GENESIIS]","userId":"13675050694185277063"},"user_tz":-330},"id":"Bcf-D7unvoId","outputId":"98be5892-c721-44b7-a25b-ad7a85a8b501"},"outputs":[],"source":["pip install split-folders\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":426},"executionInfo":{"elapsed":35586,"status":"error","timestamp":1719812214880,"user":{"displayName":"Dilshani RUBASINGHE [GENESIIS]","userId":"13675050694185277063"},"user_tz":-330},"id":"24w1FwxKzpY4","outputId":"c4ed2c49-182f-49dd-cdb6-a188cfce85d9"},"outputs":[],"source":["import os\n","import splitfolders  \n","\n","# Define the directories\n","OUTPUT_DIR = \"/content/drive/MyDrive/Semantic_segmentation_dataset2/output/github\"  # actual path where the patches are stored\n","\n","# Print the current working directory\n","print(\"Current working directory:\", os.getcwd())\n","\n","# Define the input folder using the actual OUTPUT_DIR path\n","patches_img_dir = OUTPUT_DIR\n","input_folder = os.path.abspath(patches_img_dir.strip())  \n","print(\"Input folder:\", input_folder)\n","\n","# Check if the input folder exists\n","if not os.path.exists(input_folder):\n","    raise ValueError(f\"The provided input folder '{input_folder}' does not exist.\")\n","\n","# Update this with the correct root directory path\n","root_directory = \"/content/drive/MyDrive/Semantic_segmentation_dataset2/split_folder\"\n","output_folder = os.path.join(root_directory, \"train_val_test\")\n","print(\"Output folder:\", output_folder)\n","\n","# Create the output folder if it doesn't exist\n","os.makedirs(output_folder, exist_ok=True)\n","\n","# Split the dataset into training and validation sets\n","splitfolders.ratio(input_folder, output=output_folder, seed=42, ratio=(.8, .2), group_prefix=None, move=False)  \n","# Define training and validation directories\n","train_dir = os.path.join(output_folder, \"train\")\n","val_dir = os.path.join(output_folder, \"val\")\n","\n","\n","print(\"Training directory:\", train_dir)\n","print(\"Validation directory:\", val_dir)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y9P0oId8kz9_"},"outputs":[],"source":["x_train_dir = os.path.join(train_dir, \"images\")\n","y_train_dir = os.path.join(train_dir, \"masks\")\n","\n","x_val_dir = os.path.join(val_dir, \"images\")\n","y_val_dir = os.path.join(val_dir, \"masks\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Su5pBXC6k3Y3"},"outputs":[],"source":["# helper function for data visualization\n","def visualize(**images):\n","    n = len(images)\n","    plt.figure(figsize=(16, 5))\n","    for i, (name, image) in enumerate(images.items()):\n","        plt.subplot(1, n, i + 1)\n","        plt.xticks([])\n","        plt.yticks([])\n","        plt.title(' '.join(name.split('_')).title())\n","        plt.imshow(image)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MfKWHZ4T0XZG"},"outputs":[],"source":["IMGS_DIR = \"/content/drive/MyDrive/Semantic_segmentation_dataset2/images\"\n","MASKS_DIR = \"/content/drive/MyDrive/Semantic_segmentation_dataset2/masks\"\n","OUTPUT_DIR = \"/content/drive/MyDrive/Semantic_segmentation_dataset2/output/new2.tif\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wib0XmuAk-b_"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","\n","class SegmentationDataset(Dataset):\n","\n","\n","    CLASSES = ['background', 'building', 'woodland', 'water', 'road']\n","\n","    def __init__(\n","            self,\n","            images_dir,\n","            masks_dir,\n","            classes=None,\n","            augmentation=None,\n","            preprocessing=None,\n","    ):\n","        self.ids = os.listdir(images_dir)\n","        self.images = [os.path.join(images_dir, image_id) for image_id in self.ids]\n","        self.masks = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n","\n","        # convert str names to class values on masks\n","        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n","\n","        self.augmentation = augmentation\n","        self.preprocessing = preprocessing\n","\n","    def __getitem__(self, i):\n","\n","        # read data\n","        image = cv2.imread(self.images[i])\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        image = image / 255\n","        mask = cv2.imread(self.masks[i], 0)\n","\n","        # extract certain classes from mask (e.g. cars)\n","        masks = [(mask == v) for v in self.class_values]\n","        mask = np.stack(masks, axis=-1).astype('float')\n","\n","        # apply augmentations\n","        if self.augmentation:\n","            sample = self.augmentation(image=image, mask=mask)\n","            image, mask = sample['image'], sample['mask']\n","\n","        # apply preprocessing\n","        if self.preprocessing:\n","            sample = self.preprocessing(image=image, mask=mask)\n","            image, mask = sample['image'], sample['mask']\n","\n","        return image, mask\n","\n","    def __len__(self):\n","        return len(self.ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":597413,"status":"ok","timestamp":1718018830297,"user":{"displayName":"Dilshani RUBASINGHE [GENESIIS]","userId":"13675050694185277063"},"user_tz":-330},"id":"bqDMtHKh8qBd","outputId":"19a2e223-eaa9-4f3e-b44c-ac57533b2a98"},"outputs":[],"source":["# Visualizing all classes in the mask\n","dataset = SegmentationDataset(x_train_dir, y_train_dir, classes=['background', 'building', 'woodland', 'water', 'road'])\n","image, mask = dataset[4] # get some sample\n","visualize(\n","    image = image,\n","    # Convert the predicted one-hot encoded mask back to normal\n","    mask = np.argmax(mask, axis=2)\n",")\n","\n","# Visualizing selected classes in the mask\n","dataset = SegmentationDataset(x_train_dir, y_train_dir, classes=['background', 'water', 'woodland'])\n","image, mask = dataset[16] # get some sample\n","visualize(\n","    image = image,\n","    # Convert the predicted one-hot encoded mask back to normal\n","    mask = np.argmax(mask, axis=2)\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":374,"status":"ok","timestamp":1718019079679,"user":{"displayName":"Dilshani RUBASINGHE [GENESIIS]","userId":"13675050694185277063"},"user_tz":-330},"id":"rzhYMJZ3-eF9","outputId":"6ce48b80-25a4-4881-ace2-c887c91f94b0"},"outputs":[],"source":["import albumentations as album\n","\n","def get_training_augmentation():\n","    train_transform = [\n","        album.HorizontalFlip(p=0.5),\n","        album.VerticalFlip(p=0.5),\n","        \n","    ]\n","    return album.Compose(train_transform)\n","\n","\n","def to_tensor(x, **kwargs):\n","    return x.transpose(2, 0, 1).astype('float32')\n","\n","\n","def get_preprocessing(preprocessing_fn):\n","   \n","    _transform = [\n","        album.Lambda(image=preprocessing_fn),\n","        album.Lambda(image=to_tensor, mask=to_tensor),\n","    ]\n","    return album.Compose(_transform)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qPSDeWmoAzfi"},"outputs":[],"source":["# Visualize resulted augmented images and masks\n","\n","augmented_dataset = SegmentationDataset(\n","    x_train_dir,\n","    y_train_dir,\n","    augmentation=get_training_augmentation(),\n","    classes=['background', 'building', 'woodland', 'water', 'road'],\n",")\n","\n","for i in range(3):\n","    image, mask = augmented_dataset[5123]\n","    visualize(image=image, mask=np.argmax(mask, axis=2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NdNZbY4qDLty"},"outputs":[],"source":["import torch\n","import segmentation_models_pytorch as smp\n","import segmentation_models_pytorch.utils\n","\n","BATCH_SIZE = 16\n","ENCODER = 'efficientnet-b0'\n","ENCODER_WEIGHTS = 'imagenet'\n","CLASSES = ['background', 'building', 'woodland', 'water']   # not training on 'road' class since it's instances in the data is too less\n","ACTIVATION = 'softmax2d'    # could be None for logits or 'softmax2d' for multiclass segmentation\n","DEVICE = 'cuda'\n","EPOCHS = 50\n","\n","# create segmentation model with pretrained encoder\n","model = smp.Unet(\n","    encoder_name=ENCODER,\n","    encoder_weights=ENCODER_WEIGHTS,\n","    classes=len(CLASSES),\n","    activation=ACTIVATION,\n",")\n","\n","preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UKnKzF1SDQUd"},"outputs":[],"source":["train_dataset = SegmentationDataset(\n","    x_train_dir,\n","    y_train_dir,\n","    augmentation=get_training_augmentation(),\n","    preprocessing=get_preprocessing(preprocessing_fn),\n","    classes=CLASSES,\n",")\n","\n","val_dataset = SegmentationDataset(\n","    x_val_dir,\n","    y_val_dir,\n","    preprocessing=get_preprocessing(preprocessing_fn),\n","    classes=CLASSES,\n",")\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","valid_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OtXPf2lcEHGT"},"outputs":[],"source":["\n","\n","loss = smp.utils.losses.DiceLoss()\n","metrics = [\n","    smp.utils.metrics.IoU(threshold=0.5)\n","]\n","\n","optimizer = torch.optim.Adam([\n","    dict(params=model.parameters(), lr=0.0003),\n","])\n","\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fk8YL_THEI8a"},"outputs":[],"source":["# create epoch runners\n","train_epoch = smp.utils.train.TrainEpoch(\n","    model,\n","    loss=loss,\n","    metrics=metrics,\n","    optimizer=optimizer,\n","    device=DEVICE,\n","    verbose=True,\n",")\n","\n","valid_epoch = smp.utils.train.ValidEpoch(\n","    model,\n","    loss=loss,\n","    metrics=metrics,\n","    device=DEVICE,\n","    verbose=True,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NmRhwL6RFCSx"},"outputs":[],"source":["max_score = 0\n","\n","for i in range(0, EPOCHS):\n","\n","    print('\\nEpoch: {}'.format(i))\n","    train_logs = train_epoch.run(train_loader)\n","    valid_logs = valid_epoch.run(valid_loader)\n","\n","    # save model\n","    if max_score < valid_logs['iou_score']:\n","        max_score = valid_logs['iou_score']\n","        torch.save(model, f'/content/drive/MyDrive/Colab Notebooks/Personal_Projects/Image Segmentation/Landcover Semantic Segmentation/landcover_unet_{ENCODER}_epochs{i}_patch{patch_size}_batch{BATCH_SIZE}.pth')\n","        print('Model saved!')\n","\n","    scheduler.step(valid_logs['dice_loss'])"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNneLU3ZPi4luSqNfGhz/Fi","gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
